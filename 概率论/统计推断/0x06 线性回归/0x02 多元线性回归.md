## 多元线性回归

因变量为$$y$$, 有$$k$$个自变量分别为$$x_1, x_2, \cdots, x_k$$, 描述$$y$$如何依赖于这$$k$$个自变量和**误差项**$$\varepsilon$$的方程, 称为**多元回归模型**:

$$y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k+\varepsilon$$

对于误差项$$\varepsilon$$有如下的假定:

- 误差项$$\varepsilon$$是一个随机变量, 期望值为0
- 对于任意的自变量组合, $$\varepsilon$$对应的方差$$\sigma^2$$相同
- $$\varepsilon$$服从正态分布, 且任意一组自变量对应的误差相互独立

因此, 对应的**多元回归方程**为:

$$E(y)=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_kx_k$$

对应的**估计的多元回归方程为**:

$$\hat{y}=\hat\beta_0+\hat\beta_1x_1+\hat\beta_2x_2+\cdots+\hat\beta_kx_k$$

$$\hat\beta$$是$$\beta$$的估计值, 称为**偏回归系数**. $$\hat{y}$$是因变量$$y$$的估计值.

参数且求解方法仍然是**最小二乘法**.

## 回归方程的拟合优度

#### 多重判定系数

多重判定系数是多元回归中**回归平方和**占**总平方和**的比例, 这点与一元回归一样, 也是度量**拟合**程度的一个**统计量**. 仍然符合:

$$SST=SSR+SSE$$

$$SST=\sum (y_i-\bar{y})^2=\sum (y_i-\hat{y}_i)^2 + \sum(\hat{y}_i-\bar{y})^2=SSR+SSE$$

只不过这里$$y$$的计算是与$$k$$个自变量相关的.

但需要注意的是, **自变量个数**的增加会影响到因变量中被**估计的回归方程**所解释的变差大小. 当自变量增加时, 预测误差会变小, 从而减小了**SSE**, 增大了**SSR**, 从而使多重判定系数$$R^2$$被高估. 因此使用**调整的多重判定系数**$$R^2_{\alpha}$$, 考虑了样本量和模型中自变量的数量对最终结果的影响:

$$R^2_{\alpha}=1 - (1 - R^2)(\frac{n-1}{n-k-1})$$

#### 估计标准误差

对应于一元回归, 多元回归中的轨迹标准误差即误差项$$\varepsilon$$方差的$$\sigma^2$$的一个估计值为:

$$s_e=\sqrt{\frac{\sum (y_i-\hat{y}_i)^2}{n-k-1}}=\sqrt{\frac{SSE}{n-k-1}}=\sqrt{MSE}$$

## 显著性检验

#### 线性关系检验

**线性关系检验**是检验因变量$$y$$与$$k$$个自变量之间的关系是否显著, 称为**总体显著性检验**.

- 提出假设:

  $$H_0:\beta_1=\beta_2=\cdots=\beta_k=0$$

  $$H_1: \beta_1,\beta_2,\cdots,\beta_k$$中至少有一个不为0

- 计算检验统计量:

  $$F=\frac{SSR/k}{SSE/(n-k-1)} \sim F(k, n-k-1)$$

- 做出统计决策:

  对于显著性水平$$\alpha$$, 如果$$F \gt F_{\alpha}(k, n-k-1)$$则拒绝原假设, 认为**至少有一个自变量与因变量关系显著**.

#### 回归系数检验和推断

每次对于一个系数$$\beta_i$$进行检验, 需要注意控制检验系数的个数, 避免**第I类错误**犯过多次.

- 提出假设:

  $$H_0: \beta_i=0$$

- 计算检验统计量$$t$$:

  $$t_i=\frac{\hat\beta_i}{s_{\hat\beta_i}} \sim t(n-k-1)$$

  $$s_{\hat\beta_i}$$是回归系数$$\hat\beta_i$$抽样分布的标准差:

  $$s_{\hat\beta_i}=\frac{s_e}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$

- 做出统计决策:

  对于显著性水平$$\alpha$$, 如果$$|t| \gt t_{\alpha/2}(n-k-1)$$, 则拒绝原假设.

## 多重共线性

**多重共线性**指的是, 当回归模型中两个或两个以上的自变量彼此相关时, 回归模型中存在多重共线性. 变量之间高度相关时, 会使回归结果混乱, 甚至完全错误. 表现为:

- 线性关系检验显著, 但只有很少的系数回归系数检验显著. 这是因为自变量对因变量的共现相互重叠了, 且只出现在某些变量中
- 对**参数估计值**的**正负号**产生影响, 有可能使估计值与实际值正负相反.

#### 多重共线性的判别

如果出现以下情况, 说明可能存在多重共线性:

- 模型中各对自变量之间显著相关
- 模型的**线性关系检验**($$F$$检验)显著时, 几乎所有回归系数$$\beta_i$$的$$t$$检验却不显著
- 回归系数的正负号与预期的相反
- 某个自变量的**容忍度**$$1-R^2_i$$越小, 多重共线性越严重. 其中$$R_i$$是以这个自变量为因变量, 其他$$k-1$$个自变量仍为自变量, 得到的线性回归模型的判定系数

#### 多重共线性的处理

解决方法有:

- 将一个或多个相关的自变量从模型中剔除, 使保留的自变量尽可能不相关
- 如果不删除现有的自变量, 应保证对$$y$$值的推断(预测)应当**限制在自变量样本值的范围内**

