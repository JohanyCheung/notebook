PCA(Principal Component Analysis, 主成份分析)通过**线性变换**, 将原始数据变化为一组**各维度线性无关**的表示, 可用于提取数据的**主要特征分量**, 常用于高维数据的降维. 关于PCA算法的数学原理, 在[PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)这篇文章中讲解的非常好! 下面就是关于这篇文章的简单概括.

## PCA的数学原理

#### 内积与投影

对于两个长度相同的向量$$\mathbf{a}$$和$$\mathbf{b}$$内积, 就是将两个向量映射成一个实数.

$$\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}||\mathbf{b}|\cos(\theta)$$

其中$$\theta$$是两个向量的夹角.

如果令向量$$\mathbf{b}$$的**模**为1, 即$$|\mathbf{b}|$$, 则上式为:

$$\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}|\cos(\theta)$$

即两个向量的内积值等于向量$$\mathbf{a}$$在**单位向量**$$\mathbf{b}$$所在直线**投影**的**矢量长度**. 注意这里是矢量长度, 是有正负之分的, 正负跟夹角的程度有关.

投影长度就是内积的**几何解释**.

#### 基与线性变化的矩阵表示

线性代数在几何角度上的解释. 矩阵的乘法代表着线性变换, 与每个基向量为一行组成的矩阵相乘, 将向量表示为这些基向量为坐标轴的新空间中的表示.

#### 方差与协方差

对于一个特征, 通过所有样本计算得到的**方差**表示其**分散程度**.

对于两个特征, 评价其**相关性**, 通过其协方差得到的一个值表示.

#### 协方差与内积

如果两个特征向量, 每个向量中的元素均值都为0, 则计算两个向量之间的协方差就等价于两个向量的内积再除以向量长度:

$$Cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_ib_i}$$

#### 降维问题的优化目标

找到$$k$$($$k$$为降维后向量的长度, 应当小于原来向量的长度)个方向, 即$$k$$个**单位正交基**, 使得**原始数据变换到这组基上后, 各特征两两间协方差为0，而特征的方差则尽可能大**. 在正交基的约束下, 两两特征之间的协方差为0, 即没有相关性. 而正交基上的方差尽可能大, 保证了降维之后数据的分散, 尽可能地保留了原始数据的信息.

`PCA`降维的结果直观表现就是**原矩阵经过一个矩阵线性变换后得到的结果**, 这个线性变换矩阵就是我们要求解的目标, 是**一组正交基按行组成的**.

#### 协方差矩阵

协方差矩阵很好地将特征的方差与特征之间的协方差统一在了一起. 假设我们只有a和b两个特征, 那么我们将它们按行组成矩阵$$X$$:

$$X=\begin{pmatrix}   a_1 & a_2 & \cdots & a_m \\   b_1 & b_2 & \cdots & b_m \end{pmatrix}$$

然后用$$X$$乘以$$X$$的转置, 并乘上系数$$\frac{1}{m}$$:

$$\frac{1}{m}XX^\mathsf{T}=\begin{pmatrix}   \frac{1}{m}\sum_{i=1}^m{a_i^2}   & \frac{1}{m}\sum_{i=1}^m{a_ib_i} \\   \frac{1}{m}\sum_{i=1}^m{a_ib_i} & \frac{1}{m}\sum_{i=1}^m{b_i^2} \end{pmatrix}$$

得到的就是**协方差矩阵**, 是一个**方阵**, 边长为特征的数量, 这里为2. 协方差矩阵的对角线上就是对应的每个特征的方差, 其他元素就是两两特征之间的协方差.

**设我们有m个n维数据记录, 将其按列排成n乘m的矩阵X, 设$$\frac{1}{m}XX^\mathsf{T}$$, 则C是一个对称矩阵, 其对角线分别个各个字段的方差, 而第i行j列和j行i列元素相同, 表示i和j两个字段的协方差.**

#### 协方差矩阵对角化

按照我们降维追求的目标, 等价于**将协方差矩阵对角化**:

- 特征之间不相关, 选择正交基, 相互之间协方差为0: 除了对角线其他元素为0
- 单个特征保持尽可能多的信息, 方差尽可能大: 在对角线上将元素按大小从上到下排列

分析**原矩阵**与**基变换后矩阵协方差矩阵**的关系:

原矩阵$$X$$对应的协方差矩阵为$$C$$, $$P$$即是一组基按行组成的线性变换矩阵. $$Y=PX$$即是原矩阵经过现行变换后数据的表示, 其对应的协方差矩阵为$$D$$, 推导$$D$$与$$C$$之间的关系:

$$\begin{array}{l l l}   D & = & \frac{1}{m}YY^\mathsf{T} \\     & = & \frac{1}{m}(PX)(PX)^\mathsf{T} \\     & = & \frac{1}{m}PXX^\mathsf{T}P^\mathsf{T} \\     & = & P(\frac{1}{m}XX^\mathsf{T})P^\mathsf{T} \\     & = & PCP^\mathsf{T} \end{array}$$

要找的线性变换矩阵$$P$$不是别的, 而是能让原始协方差矩阵对角化的$$P$$. 优化目标变成了**寻找一个矩阵$$P$$, 满足$$PCP^\mathsf{T}$$是一个对角矩阵, 并且对角元素按从大到小依次排列, 那么$$P$$的前$$K$$行就是要寻找的基, 用$$P$$的前$$K$$行组成的矩阵乘以$$X$$就使得$$X$$从$$N$$维降到了K维并满足上述优化条件**.

接下来就是如何对角化协方差矩阵$$C$$的问题. 由于协方差矩阵$$C$$是一个**对称矩阵**, 在线性代数上, 实对称矩阵有一系列非常好的性质:

- 实对称矩阵不同特征值对应的特征向量必然正交
- 设特征向量$$\lambda$$的重数为$$r$$, 则必然存在$$r$$个线性无关的特征向量对应于$$\lambda$$, 因此可以将这$$r$$个特征向量单位正交化

由上面两条可知, 一个$$n$$行$$n$$列的实对称矩阵一定可以找到$$n$$个单位正交特征向量, 假设这$$n$$个特征向量为$$e_1,e_2,\cdots,e_n$$, 我们将其按列组成矩阵$$E=\begin{pmatrix}   e_1 & e_2 & \cdots & e_n \end{pmatrix}$$, 则对协方差矩阵$$C$$有如下结论:

$$E^\mathsf{T}CE=\Lambda=\begin{pmatrix}   \lambda_1 &             &         & \\               & \lambda_2 &         & \\               &             & \ddots & \\               &             &         & \lambda_n \end{pmatrix}$$

其中$$\Lambda$$为对角矩阵, 其对角元素为各特征向量对应的特征值(可能有重复).

到这里, 我们发现我们已经找到了需要的矩阵$$P$$:

$$P=E^\mathsf{T}$$

$$P$$是协方差矩阵的特征向量单位化后按行排列出的矩阵, 其中每一行都是$$C$$的一个特征向量. 如果设$$P$$按照$$\Lambda$$中特征值的从大到小, 将特征向量从上到下排列, 则用$$P$$的前$$K$$行组成的矩阵乘以原始数据矩阵$$X$$，就得到了我们需要的降维后的数据矩阵$$Y$$.

以上就是**PCA**算法的数学原理.

## PCA总结

#### 本质

PCA本质上是将方差最大的方向作为主要特征, 并且在各个正交方向上将数据离相关, 也就是让它们在不同正交方向上没有相关性.

可以**很好的解除线性相关**.

#### 限制

- 它可以很好的解除线性相关, 但是对于高阶相关性就没有办法了. 解决方法: **Kernel PCA**, 通过Kernel函数将非线性相关转为线性相关.
- PCA假设数据各主特征是分布在正交方向上, 如果在非正交方向上存在几个方差较大的方向, PCA的效果就大打折扣了.

#### 特点

PCA是一种无参数技术, 也就是说面对同样的数据, 结果都一样. 没有主观参数的介入, PCA便于通用实现, 但是本身无法个性化的优化.

