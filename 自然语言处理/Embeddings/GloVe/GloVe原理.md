## 引入

常用的两种对词进行向量化的方法族都有自身的缺点:

- **global matrix factorization methods**(全局统计矩阵分解法): 例如**positive pointwise mu- tual information(PPMI)**方法. 类似于这种方法, 充分考虑了全局的统计信息(词频等), 但没有考虑单词的局部情况, 导致在**单词推理**(word analogy)任务中表现很差.
- **Shallow Window-Based Methods**(浅层窗口方法): 例如word2vec中的CBOW或Skip-gram(Skip-Gram with Negative Sampling, **SGNG**)方法. 这种方法在单词推理的任务中表现很好, 但是没有使用到预料的统计信息, 只是使用到了窗口限定的局部信息.

其中的**单词推理**任务举个例子就是word2vec论文中经典的`man->woman`的关系推理出`king->?`中的未知单词是`queen`. 而且word2vec产生的词向量, 还有着$$e_{man}-e_{woman} \approx e_{king}-e_{queen}$$的关系, 这可以被定义为在词义上有线性表现(**linear directions of meaning**).

文章认为这个属性是词向量化模型必须具备的属性, 然后提出了使用对数[双线性](http://zhoufazhe2008.blog.163.com/blog/static/6332686920100744237926/)(log-bilinear)回归模型来保证这种属性. 通过一个加权最小平方模型(weighted least squares model), 通过语料全局的**word-word co-occurrence**的**次数**来训练这个模型, 这样就使用到了全局的统计信息. 而且词与词之间的**co-occurrence**通过窗口限制, 也考虑了局部, 因此使用了语料中更多的信息, 理论上向量化效果应当更好.

## Glove模型

Global Vectors(**GloVe**)模型使用语料全局统计信息, 并且生成的词向量, 并且词向量与词义有着希望的对应关系.

定义符号:

- $$X$$: matrix of word-word co-occurrence counts
- $$X_{ij}$$: 单词$$j$$在单词$$i$$的上下文中出现的次数
- $$X_i=\sum\limits_{k}X_{ik}$$: 所有单词在单词$$i$$的上下文中出现的次数
- $$P_{ij}=P(j|i)=\frac{X_{ij}}{X_{i}}$$: 单词$$j$$出现在单词$$i$$的上下文中的概率

考虑几个例子:

假设有两个单词$$i=ice$$, $$j=steam$$. 我们认为, 这两个单词的相关性, 可以通过第三个探针单词$$k$$来体现, 具体来说是通过比例$$P_{ik}/P_{jk}$$来体现, 如何体现呢?

例如说单词$$k$$与$$i$$很相关, 而与$$j$$不相关, 例如$$k=solid$$, 这样$$P_{ik}/P_{jk}$$的值就会很大; 再比如单词$$k$$与$$i$$不相关, 而与$$j$$很相关, 例如$$k=gas$$, $$P_{ik}/P_{jk}$$值就会很小; 如果单词$$k$$同时与两个单词都很相关, 例如$$k=water$$, 或者同时与两个单词都不相关, 例如$$k=fashion$$, 那个$$P_{ik}/P_{jk}$$值就会接近于1.

因此, 相对于使用原始的条件概率$$P_{ij}$$, 这个比例的大小就很能很好的区分相关与否的单词.

定义$$ratio_{i,j,k}=P_{ik}/P_{jk}$$, 这个值与$$i$$, $$j$$, $$k$$三个单词相关. 这个概率目前使用全局共现频率统计计算得到的, 如果我们可以通过这三个单词的词向量计算得到, 那么就可转换成:

$$F(w_i,w_j,\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}$$

这就是我们希望在词的**向量空间**中的运算来表示这个概率.

下面就是把上式转换成我们可以计算的形式的过程:

- 我们希望词向量具有线性特性, 那么词向量之间的差向量就能很好的定义两个单词之间的差异性. 由于我们要考察$$i$$和$$j$$之间的相关性, 所以上式可以转换为:

  $$F(w_i-w_j,\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}$$

- 注意到等式右侧是标量, 左侧的变量是两个向量, 因此很自然的想到通过**内积**的方式进行转换, 即函数内部应该有向量之间的内积运算, 因此也就是这个内积就可以做自变量. 而且内积也能很好的保持向量之间的线性, 得到:

  $$F((w_i-w_j)^T\tilde{w}_k)=\frac{P_{ik}}{P_{jk}}$$

- 考虑到使用了单词之间的**co-occurrence**矩阵, 因此对于一个中心词和其对应的某个上下文词, 这两个词的角色是可以相互转换的, 即应表现出**对称性**. 又考虑到使用词向量表示$$P_{ik}$$, 而且没有顺序区别, 点积是很好的形式, 因此:

  $$F((w_i-w_j)^T\tilde{w}_k)=\frac{F(w_i^T\tilde{w}_k)}{F(w_j^T\tilde{w}_k)}$$

  其中:

  $$F(w_i^T\tilde{w}_k)=P_{ik}=\frac{X_{ik}}{X_i}$$

  而对于$$F((w_i-w_j)^T\tilde{w}_k)=\frac{F(w_i^T\tilde{w}_k)}{F(w_j^T\tilde{w}_k)}$$这种形式, 观察左侧的相减转换成了右侧相除的形式, 容易联想到**幂函数**的形式. 因此使$$F=\exp$$, 则$$F(w_i^T\tilde{w}_k)=P_{ik}=\frac{X_{ik}}{X_i}$$一式转换为:

  $$w_i^T\tilde{w}_k=\log(X_{ik})-\log(X_i)$$

  注意到$$X_{ik}=X_{ki}$$, 因此上面等式中三项只有$$\log(X_i)$$没有对称性. 又由于$$\log(X_i)$$是关于$$i$$的一个函数, 所以将其即为一个**偏置项**$$b_i$$, 为了对称性同时加入单词$$k$$的偏置项$$\tilde{b}_k$$, 式子转换成:

  $$w_i^T\tilde{w}_k+b_i+\tilde{b}_k=\log(X_{ik})$$

- 我们希望上式的两端尽可能相等, 因此用差的平方来衡量差距. 而考虑到$$X_{ik}$$两个单词共现的频率不同, 采用加权的方法, 对词典中所有项进行衡量, 因此得到最终的损失函数:

  $$L=\sum\limits_{i,j=1}^{V}f(X_{ij})(w_i^T\tilde{w}_j+b_i+\tilde{b}_j-\log{X_{ij}})^2$$

  其中$$V$$是字典中词的数量. 而权重函数$$f$$需要考虑如下几点:

  - $$f(0)=0$$, 且当$$x\to{0}$$时, $$\lim\limits_{x\to{0}}f(x)\log^2x$$是有限值
  - $$f(x)$$应当是非递减函数, 这样很少共现的情况就不会被考虑过重
  - $$f(x)$$对于很大的自变量$$x$$其值也不能过大, 这样就不会过度考虑频繁共现的情况

  论文中最终选择:

  $$f(x)=\begin{cases} (x/x_{max})^{\alpha}, \quad \text{if } x < x_{max} \\ 1, \quad \text{otherwise} \end{cases}$$

  其中$$x_{max}=100$$, $$\alpha=0.75$$即可.

## GloVe训练

需要选择的超参数有:

- 上下文窗口
  - 窗口长度
  - 单向/双向窗口
- $$x_{max}$$
- $$\alpha$$
- 学习率
- 词向量的维度
- 迭代的轮数

而在训练过程中, 两个单词, 中心词词向量$$w$$和上下文词词向量$$\tilde{w}$$使用的不是同一个词向量矩阵. 但当$$X$$是对称的, 则$$W$$和$$\tilde{W}$$矩阵就是等同的. 但在训练的初始化时, 这两个嵌入参数矩阵用不同的数值进行初始化. 在最后对于每个单词最终的词向量, 从$$W+\tilde{W}$$矩阵中取出对应单词的限量. 即将两个矩阵加和在一起作为最后的结果, 这样的方法能够:

- 噪声的影响
- 过拟合

