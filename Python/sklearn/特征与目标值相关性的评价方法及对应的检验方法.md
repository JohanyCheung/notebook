## sklearn与特征筛选

`sklearn.feature_selection`子模块顾名思义, 是对特征进行选择, 也就是特征筛选的模块. 常用的方法根据具体的情况分为:

- 分类任务
  - 该特征是**二值特征**或**类别特征**:
    - `chi2`
  - 该特征是**数值型特征**, 包括连续特征和离散特征, 只要值的大小有意义即可
    - `f_classif`
  - 任何类型的特征, 需要注明特征的类型
    - `mutual_info_classif`
- 回归任务
  - 特征是**数值型特征**, 包括连续特征和离散特征, 只要值的大小有意义即可
    - `f_regression`
  - 任何类型的特征, 需要注明特征的类型
    - `mutual_info_regression`

## 基于检验方法的特征筛选

这类方法都是基于统计学的检验方法: `chi2`, `f_classif`, `f_regression`. 分别说明.

### chi2

**`chi2(X, y)`**

首先要求目标`y`是分类标签, 然后要求进行检验的特征必须是**二值特征**, 即特征值非0即1. 因为`chi2`是通过**卡方检验**来不同的目标标签对应的特征表现是否相同, 从而说明该特征对目标标签的区分能力强不强. 如果目标标签的每类中对应特征的**期望频数**和**观测频数**接近, 说明没有区别能力, 这个特征可能不能起到很好的作用. 具体的原理参考[0x01 分类数据分析基础](https://blessbingo.gitbook.io/garnet-tech/math/gai-shuai-lun/tong-ji-tui-duan/0x04-fen-lei-shu-ju-fen-xi/0x01-fen-lei-shu-ju-fen-xi-ji-chu).

如果一个特征是**类别特征**, 则需要通过`One-Hot`进行编码后使用.

### f_classif

`f_classif(X, y)`

使用**ANOVA**, 单变量的**F检验**来进行计算. 该检验方法将样本按分类标签分组, 原假设认为所有组对应的特征的均值相等, 这种情况下该特征对样本的分类就没有作用了. 因此得到的**F统计量**越大, 说明该特征的区别能力越强.

使用与分类标签和**数值型特征**. 这里的数值型指的是数值有意义的特征, 并不区分离散还是连续. 与之对应的是**类别型特征**, 即不同的数值代表不同的类别, **数值大小**没有意义.

具体原理参考[0x02 单因素方差分析](https://blessbingo.gitbook.io/garnet-tech/math/gai-shuai-lun/tong-ji-tui-duan/0x05-fang-cha-fen-xi/0x02-dan-yin-su-fang-cha-fen-xi)

### f_regression

`f_regression(X, y, center=True)`

这是针对回归问题, 对回归值和**数值型特征**进行检验.

使用的是**Univariate linear regression tests**, 即**单变量回归检验**. 检验的**线性相关性**. 因此如果一个特征与回归值的关系是非线性的, 则不能真实的表现这种关系, 这也是这种方法的局限性.

具体原理参考[0x01 一元线性回归基础](https://blessbingo.gitbook.io/garnet-tech/math/gai-shuai-lun/tong-ji-tui-duan/0x06-xian-xing-hui-gui/0x01-yi-yuan-xian-xing-hui-gui-ji-chu).


### mutual_info_classif与mutual_info_regression

`mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)`

`mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)`

这两种方法都是基于**互信息**的方法. 具体的原理涉及到四篇论文, 在`sklearn`中这两个方法对应代码的注释中有引出.

由于是基于概率的方法, 因此突破了线性的限制. 例如以回归值与数值特征的检验为例, `mutual_info_regression`方法就能体现出`f_regression`方法不能涵盖的非线性因素.

在使用的过程中需要**特别注意**`discrete_features`参数, 这个参数是这指定进行检验的`X`中包含的所有特征的类别, 其实这里的`discrete`指的就是**分类特征**与**数值特征**的区别, 并不是严格意义的离散和连续特征. 如果对于特征的类别设定错误, 或者忽略而没有设定, 那么最后得到的结果的准确性就无法保证, 甚至是错误的.
