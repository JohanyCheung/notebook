## 回归分析

**回归分析**是对**数值型自变量**和**数值型因变量**之间相关关系进行分析的方法. 变量之间存在的不确定的数量关系, 称为**相关关系(correlation)**.

在进行相关分析时, 对总体有以下的假定:

- 两个变量之间是**线性关系**
- 两个变量都是随机变量

#### 相关系数

为准确度量两个变量之间的**线性关系**强度, 需要计算**相关系数**(correlation coefficient). 相关系数是根据样本数据计算的, 度量两个变量之间**线性关系**强度的统计量.

- 如果相关系数是通过总体的全部数据计算得到的, 称为**总体相关系数**, 记为$$\rho$$
- 如果是根据样本数据计算的, 称为**样本相关系数**, 记为$$r$$

其中, **样本相关系数**$$r$$的计算公式为:

$$r=\frac{n\sum xy - \sum x \sum y}{\sqrt{n\sum x^2-(\sum x)^2} \sqrt{n\sum y^2 - (\sum y)^2}}$$

上式计算得到的相关系数称为**线性相关系数**, 也称为**皮尔逊相关系数**.

**注意**: $$r$$虽然是两个变量之间线性关系的一个度量, 但不意味着$$x$$与$$y$$之间一定有因果关系. 可以通过**显著性检验**判断两个变之间是否有相关关系.

## 相关关系的显著性检验

**总体相关系数**$$\rho$$是**未知的**, 通常是将**样本相关系数**$$r$$作为$$\rho$$的近似估计值. 而$$r$$是一个**随机变量**, 是否能根据样本相关系数, 说明总体的相关程度呢? 需要考察样本相关系数的可靠性, 也就是进行显著性检验.

#### $$r$$的抽样分布

$$r$$的抽样分布, 随着样本量的增大, 趋于**正态分布**.

因为$$r$$是在$$\rho$$为中心的周围分布的, 因此当总体相关系数$$\rho$$很小或者接近0时, 这种趋于正态分布的趋势就非常明显; 当$$\rho$$远离0时, $$r$$的抽样分布会呈现出一定的**偏态**, 除非样本量$$n$$非常大. 这是因为$$r$$取值范围在-1到1之间, 且$$r$$又在$$\rho$$的两侧分布, 当$$\rho$$的绝对值接近于1时, 所以一个方向的分布变化是有限的, 因此会产生**不对称**的线性.

#### $$r$$的显著性检验

从上面的$$r$$的抽样分布的分析可知, 如果直接使用正态分布来检验, 是有较大的风险的, 通常会采用**$$t$$检验**来进行. 具体的检验步骤如下:

- 提出假设: $$H_0:\rho=0$$
- 计算检验的统计量: $$t=|r|\sqrt{\frac{n-2}{1-r^2}} \sim t(n-2)$$
- 进行决策: 根据显著性水平$$\alpha$$和自由度$$df=n-2$$来查$$t$$分布表, 得出$$t_{\alpha/2}(n-2)$$的临界值, 若$$|t| \gt t_{\alpha/2}(n-2)$$, 则拒绝原假设$$H_0$$, 表明总体的两个变量之间存在显著的线性关系.

## 一元线性回归

相关分析的目的在于测量变量之间的关系强度, 借助**相关系数**进行测度. **回归分析**侧重于考察变量之间的数量关系, 并通过一定的数学表达式, 将这种数量关系描述出来, 确定一个或几个自变量对另一个因变量的影响程度.

回归分析可以用来解决下面几个方面的问题:

- 从一组样本数据出发, 确定变量之间的数学关系式. 这种用途是最通常的
- 对数学关系式的**可信程度**进行各种**统计检验**, 并从影响因变量的诸多自变量中找出影响显著的和不显著的
- 利用求得的数学关系式, 根据一个或几个自变量的取值来估计或预测因变量的取值, **并给出这种估计或预测的可靠程度**. 前半句是机器学习中回归模型的原理, 后半句可以通过一些统计检验方法来对预测值的可靠性程度进行度量

#### 一元线性回归模型

回归模型描述**因变量**$$y$$如何依赖于**自变量**$$x$$和**误差项**$$\varepsilon$$:

$$y=\beta_0+\beta_1x+\varepsilon$$

把$$y$$分解成**线性函数部分**$$\beta_0+\beta_1x$$和误差项$$\varepsilon$$. 线性函数部分反映了由于自变量$$x$$变化而引起的因变量$$y$$的**线性变化**, 误差项是一个随机变量, 反映了除$$x$$和$$y$$之间的线性关系外的**随机因素**对$$y$$影响, 是一种变异性.

有以下注意点:

- 对于所有的$$x$$值, $$\varepsilon$$的方差$$\sigma^2$$都相同, 因此对于任意一个$$x$$对应的$$y$$, $$y$$的方差也是$$\sigma^2$$
- 误差项$$\varepsilon$$是一个服从正态分布的随机变量, 且独立, 即$$\varepsilon \sim N(0, \sigma^2)$$. 独立性意味着对于任意$$x$$值, 所对应的$$\varepsilon$$与其他$$x$$值所对应的$$\varepsilon$$不相关

#### 回归方程

对回归模型取**期望**, 得到$$y$$的期望值$$E(y)$$所对应的方差:

$$E(y)=\beta_0+\beta_1x$$

即**$$y$$的期望值是$$x$$的线性函数**, 将描述**期望值如何依赖于自变量**的方程称为**回归方程**. 因此机器学习中的有显式公式的回归模型其实都是描述与因变量期望的关系.

#### 估计的回归方程

回归方程中的参数$$\beta_0$$, $$\beta_1$$是未知的, 需要根据样本数据去估计他们, **样本统计量**$$\hat{\beta}_0$$与$$\hat{\beta}_1$$来代替回归方程中的参数, 就得到了**估计的回归方程**:

$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x$$

#### 最小二乘估计

根据**最小误差**来确定估计参数$$\hat{\beta}$$的方法称为**最小二乘法**或**最小平方法**, 这里的误差就是指的距离的平方.

## 回归直线的拟合优度

回归直线与各观测点的接近程度称为**回归直线**对**数据**的**拟合优度**. 通过计算**判定系数**来反映拟合优度.

#### 判定系数

判定系数是对估计的回归方程拟合优度的度量.

$$y$$值取值的波动称为变差. 它来源于两个方面:

- 由自变量$$x$$取值不同造成的
- 除$$x$$外的其他因素($$x$$对$$y$$的非线性关系, 测量误差等)

对于一个具体的观测值来说, 变差的大小可以用**实际观测值**$$y$$与其**均值**$$\bar{y}$$之差来表示(注意这里的均值指的是所有可能取值$$x$$对应的$$y$$值的均值, 即所有样本的均值). $$n$$个观测值的总变差由这些**离差的平方和**来表示, 称为**总平方和(SST)**:

$$SST=\sum (y_i-\bar{y})^2$$

可以分解为:

$$SST=\sum (y_i-\bar{y})^2=\sum (y_i-\hat{y}_i)^2 + \sum(\hat{y}_i-\bar{y})^2$$

即:

$$SST=SSR+SSE$$

其中$$\sum(\hat{y}_i-\bar{y})^2$$是回归值$$\hat{y}_i$$与均值$$\bar{y}$$的离差平方和, 可以看做是由于自变量$$x$$的变化引起的因变量$$y$$的变化, 反映了$$y$$的总变差中由于$$x$$和$$y$$之间的线性关系引起的变化部分, 是可以由回归直线来解释的变差部分, 称为**回归平方和(SSR)**:

$$SSR=\sum(\hat{y}_i-\bar{y})^2$$

$$\sum (y_i-\hat{y}_i)^2$$是各观测点与回归值的残差的平方和, 反映了除了$$x$$对$$y$$的线性影响之外的其他因素引起的变化部分, 是不能由回归直线来解释的变差部分, 记为**残差平方和/误差平方和(SSE)**:

$$SSE=\sum (y_i-\hat{y}_i)^2$$

因此, $$SSR/SST$$越大, 回归直线拟合的越好. **回归平方和**占**总平方和**的比例称为**判定系数**, 记为$$R^2$$:

$$R^2=\frac{SSR}{SST}=\frac{\sum(\hat{y}_i-\bar{y})^2}{\sum (y_i-\bar{y})^2}=1 - \frac{\sum (y_i-\hat{y}_i)^2}{\sum (y_i-\bar{y})^2}$$

判定系数$$R^2$$测度了**回归直线对观测数据的拟合程度**, 取值范围是$$[0,1]$$. 值越大说明拟合的效果越好.

**相关系数**$$r$$是判定系数的**平方根**, 相关系数$$r$$与回归系数$$\hat{\beta}_1$$的正负号是相同的. 相关系数也从另一种角度说明了回归直线的拟合程度.

#### 估计标准误差

**残差平方和**可以说明实际观测值与回归估计值之间的差异程度. **估计标准误差**就是度量个实际观测点在直线周围散步状况的一个**统计量**, 它是**均方残差(MSE)**的平方根, 用$$s_e$$表示:

$$s_e=\sqrt{\frac{\sum (y_i-\hat{y}_i)^2}{n-2}}=\sqrt{\frac{SSE}{n-2}}=\sqrt{MSE}$$

它反映了用估计的回归方程预测因变量$$y$$时, 预测误差的大小. $$s_e$$越小, 回归直线对各观测点的代表性越好, 进行的预测越准确.

## 显著性检验

根据样本数据建立了估计方程后, 还需要通过检验, 才能判断该方程是否真实地反映了变量$$x$$和$$y$$之间的关系. 只有通过了检验, 才能拿来进行预测.

回归分析中的显著性检验包含两方面的内容:

- 线性关系检验
- 回归系数检验

#### 线性关系检验

**线性关系检验**是检验自变量和因变量之间的线性关系是否显著. 首先需要构造统计量, 这里的统计量是用**回归平方和SSR**和**残差平方和SSE**为基础构造的.

将SSR除以对应的自由度, 得到**均方回归MSR**; 将SSE除以对应的自由度, 得到**均方残差MSE**. 此时的原假设为$$H_0: \beta_1=0$$, 即两个变量之间的线性关系不显著. 则有:

$$F=\frac{SSR/1}{SSE/(n-2)}=\frac{MSR}{MSE} \sim F(1, n-2)$$

这里使用的是$$F$$统计量. 根据显著性水平$$\alpha$$找到临界值$$F_{\alpha}$$, 如果$$F \gt F_{\alpha}$$, 则说明两个变量之间的关系式显著的.

#### 回归系数检验

回归系数的显著性检验是要检验自变量对因变量的影响是否显著. 回归系数的显著性检验是检验回归系数$$\beta_1$$是否等于0, 因此原假设为$$H_0: \beta_1=0$$.

接下来是构造统计量. 统计证明$$\hat{\beta}_1$$服从正态分布, 数学期望为$$E(\hat{\beta}_1)=\beta_1$$, 标准差为:$$\sigma_{\hat\beta_1}=\frac{\sigma}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$, 其中$$\sigma$$是误差项$$\varepsilon$$的标准误差. 由于$$\sigma$$是未知的, 就用它的估计量$$s_e$$来代替, 代入后得到估计量:

$$s_{\hat\beta_1}=\frac{s_e}{\sqrt{\sum x^2_i - \frac{1}{n}(\sum x_i)^2}}$$

因此, 构造出用于检验回归系数的$$t$$统计量:

$$t=\frac{\hat\beta_1-\beta_1}{s_{\hat\beta_1}}$$

服从自由度为$$n-2$$的$$t$$分布. 由于原假设中$$\beta_1=0$$, 因此构造的统计量为$$t=\frac{\hat\beta_1}{s_{\hat\beta_1}}$$, 再根据显著性水平$$\alpha$$, 找到相应的临界值$$t_{\alpha/2}$$, 如果$$|t| \gt t_{\alpha/2}$$, 否认原假设, 认为自变量对因变量之间的关系是显著的.

## 利用回归方程进行预测

回归模型经过各种检验表明合理性后, 就可以用来进行预测, 即预测因变量$$y$$的期望值$$E(y)$$. 对于一个给定的$$x_0$$, 通常我们使用**点估计**, 给出对应的估计值$$E(y_0)$$. 这里要详细说明的是区间估计.

#### 回归预测的区间估计

利用**估计的回归方程**, 对于$$x$$的一个特定值$$x_0$$, 求出$$y$$的**一个估计值的区间**, 称为**区间估计**. 区间估计也有两种类型:

- **置信区间**估计: 对一个给定的$$x_0$$, 求出$$y$$的平均值的估计区间, 即$$E(y)$$的区间
- **预测区间**估计: 对一个给定的$$x_0$$, 求出$$y$$的一个**个别值**的估计区间

#### $$y$$的平均值的置信区间估计

$$x_0$$为自变量$$x$$的一个给定值, $$E(y_0)$$是给定$$x_0$$时因变量$$y$$的真实平均值或期望值, $$\hat{y}_0$$是$$E(y)$$的估计值.

期望估计值$$\hat{y}_0$$不能精确地等于$$E(y_0)$$, 因此要用$$\hat{y}_0$$来推断$$E(y_0)$$. 这个过程需要考虑估计的回归方程得到的$$\hat{y}_0$$的标准, 用$$s_{\hat{y}_0}$$表示:

$$s_{\hat{y}_0}=s_e\sqrt{\frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}}$$

整体符合$$t$$分布, 因此在$$1-\alpha$$的置信水平下的置信区间为:

$$\hat{y}_0 \pm t_{\alpha/2}s_e\sqrt{\frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}}$$

可以看出, 当$$x_0=\bar{x}$$时, 标准差最小, 置信区间最窄, 估计是最准确的; 偏离$$\bar{x}$$越远, 估计就越不好.

#### $$y$$个别值的预测区间估计

对于给定值$$x_0$$, 求出$$y$$的一个**个别值**$$y_0$$的区间估计. 仍然是符合$$t$$分布, 但是此时的标准差变为了:

$$s_{ind}=s_e\sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}}$$

因此此时的预测区间变为了:

$$\hat{y}_0 \pm t_{\alpha/2}s_e\sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}}$$

#### 两者区别

可以看出, 对个别值的区间估计会比对期望的估计对应的区间宽度会更宽, 说明估计**平均值**比估计**特定值/个别值**会更精确.

