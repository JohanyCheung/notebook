## 样本分类不平衡的问题

- 更换**损失函数**, 使得损失函数尽量少地考虑负样本, 更多的考虑正样本, 即正样本带来的损失要远大于负样本.
  - [focal loss](https://arxiv.org/abs/1708.02002)
  
## 过拟合问题

- 使用特殊的损失函数
  - [robust category cross entropy](https://kexue.fm/archives/4493), 降低分类问题的自信程度, 过于自信的网络对于噪音也会很极端的分类, 通过引入每个样本对于每个类别的损失, 降低网络的自信程度, 减少过拟合的现象.

## Embedding表征聚类性不强

- 对于分类问题, 有时更关注样本最后的特征向量$$\mathbf{x}$$, 而不是最后类别. 因为在应用中, 我们对最后的分类没有兴趣, 而是通过训练分类任务得到一个表征模型, 然后进行下一步的任务.
- 但实际上, 如果使用**softmax**作为最后损失的计算方法, 则整个模型得到的特征不一定具有**聚类特性**, 相反, 它们会尽量布满整个空间(参考center loss的相关论文和文章).
- 为了使训练之后的表征具有聚类特性, 使用**center loss**, 对原来的**cross entropy**增加一个聚类的**惩罚项**:
  - $$loss = - \log\frac{e^{\boldsymbol{W}_y^{\top}\boldsymbol{x}+b_y}}{\sum\limits_i e^{\boldsymbol{W}_i^{\top}\boldsymbol{x}+b_i}} + \lambda \Big\Vert \boldsymbol{x}-\boldsymbol{c}_y \Big\Vert^2$$
  - 第二项就是额外的惩罚项, 它给每个类定义了**可训练的中心**$$\mathbf{c}$$, 要求每个类要跟各自的中心靠得很近.
- [Embedding层的妙用](https://kexue.fm/archives/4493)
